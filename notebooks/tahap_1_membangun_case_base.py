# -*- coding: utf-8 -*-
"""Tahap 1 Membangun Case Base.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WUfvdyECpRPuvz3WWp7kh3DcUZWkMNr9

# **Tahap 1 - Membangun Case Base**

---
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pandas requests beautifulsoup4 pdfminer.six lxml > /dev/null 2>&1

import argparse
import io
import os
import re
import time
import urllib
from concurrent.futures import ThreadPoolExecutor, wait
from datetime import date
import pandas as pd
import requests
from bs4 import BeautifulSoup
from pdfminer.high_level import extract_text

def create_path(folder_name):
    path = os.path.join(os.getcwd(), folder_name)
    if not os.path.exists(path):
        os.makedirs(path)
    return path

def open_page(link):
    count = 0
    while count < 3:
        try:
            return BeautifulSoup(requests.get(link).text, "lxml")
        except:
            count += 1
            time.sleep(5)


def get_detail(soup, keyword):
    try:
        text = (
            soup.find(lambda tag: tag.name == "td" and keyword in tag.text)
            .find_next()
            .get_text()
            .strip()
        )
        return text
    except:
        return ""


def get_pdf(url, path_pdf):
    try:
        file = urllib.request.urlopen(url)
        file_name = os.path.basename(url)
        file_content = file.read()
        with open(f"{path_pdf}/{file_name}", "wb") as out_file:
            out_file.write(file_content)
        return io.BytesIO(file_content), file_name
    except:
        return None, None


def clean_text(text):
    text = text.replace("M a h ka m a h A g u n g R e p u blik In d o n esia\n", "")
    text = text.replace("Disclaimer\n", "")
    text = text.replace(
        "Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\n",
        "",
    )
    text = text.replace(
        "pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\n",
        "",
    )
    text = text.replace(
        "Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\n",
        "",
    )
    text = text.replace(
        "Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\n",
        "",
    )
    return text


def extract_data(link, keyword_url, path_output, path_pdf, today):
    soup = open_page(link)
    table = soup.find("table", {"class": "table"})
    judul = table.find("h2").text if table.find("h2") else ""

    nomor = get_detail(table, "Nomor")
    tingkat_proses = get_detail(table, "Tingkat Proses")
    klasifikasi = get_detail(table, "Klasifikasi")
    kata_kunci = get_detail(table, "Kata Kunci")
    tahun = get_detail(table, "Tahun")
    tanggal_register = get_detail(table, "Tanggal Register")
    lembaga_peradilan = get_detail(table, "Lembaga Peradilan")
    jenis_lembaga_peradilan = get_detail(table, "Jenis Lembaga Peradilan")
    hakim_ketua = get_detail(table, "Hakim Ketua")
    hakim_anggota = get_detail(table, "Hakim Anggota")
    panitera = get_detail(table, "Panitera")
    amar = get_detail(table, "Amar")
    amar_lainnya = get_detail(table, "Amar Lainnya")
    catatan_amar = get_detail(table, "Catatan Amar")
    tanggal_musyawarah = get_detail(table, "Tanggal Musyawarah")
    tanggal_dibacakan = get_detail(table, "Tanggal Dibacakan")
    kaidah = get_detail(table, "Kaidah")
    status = get_detail(table, "Status")
    abstrak = get_detail(table, "Abstrak")

    if klasifikasi:
        klasifikasi_lc = klasifikasi.lower()
        if not ("penipuan" in klasifikasi_lc or "penggelapan" in klasifikasi_lc):
            print(f"‚ùå Lewati: Bukan penipuan/penggelapan ‚Üí {klasifikasi}")
            return
    else:
        return

    try:
        link_pdf = soup.find("a", href=re.compile(r"/pdf/"))["href"]
        file_pdf, file_name_pdf = get_pdf(link_pdf, path_pdf)
        text_pdf = extract_text(file_pdf)
        text_pdf = clean_text(text_pdf)
    except:
        link_pdf = ""
        text_pdf = ""
        file_name_pdf = ""

    data = [
        judul,
        nomor,
        tingkat_proses,
        klasifikasi,
        kata_kunci,
        tahun,
        tanggal_register,
        lembaga_peradilan,
        jenis_lembaga_peradilan,
        hakim_ketua,
        hakim_anggota,
        panitera,
        amar,
        amar_lainnya,
        catatan_amar,
        tanggal_musyawarah,
        tanggal_dibacakan,
        kaidah,
        status,
        abstrak,
        link,
        link_pdf,
        file_name_pdf,
        text_pdf,
    ]
    result = pd.DataFrame(
        [data],
        columns=[
            "judul",
            "nomor",
            "tingkat_proses",
            "klasifikasi",
            "kata_kunci",
            "tahun",
            "tanggal_register",
            "lembaga_peradilan",
            "jenis_lembaga_peradilan",
            "hakim_ketua",
            "hakim_anggota",
            "panitera",
            "amar",
            "amar_lainnya",
            "catatan_amar",
            "tanggal_musyawarah",
            "tanggal_dibacakan",
            "kaidah",
            "status",
            "abstrak",
            "link",
            "link_pdf",
            "file_name_pdf",
            "text_pdf",
        ],
    )

    keyword_url = keyword_url.replace("/", " ")
    if keyword_url.startswith("https"):
        keyword_url = ""
    destination = f"{path_output}/putusan_ma_{keyword_url}_{today}"
    if not os.path.isfile(f"{destination}.csv"):
        result.to_csv(f"{destination}.csv", header=True, index=False)
    else:
        result.to_csv(f"{destination}.csv", mode="a", header=False, index=False)


def run_process(keyword_url, page, sort_date, path_output, path_pdf, today):
    if keyword_url.startswith("https"):
        link = f"{keyword_url}&page={page}"
    else:
        link = f"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}"
    if sort_date:
        link = f"{link}&obf=TANGGAL_PUTUS&obm=desc"

    soup = open_page(link)
    links = soup.find_all("a", {"href": re.compile("/direktori/putusan")})

    for link in links:
        extract_data(link["href"], keyword_url, path_output, path_pdf, today)


def run_scraper(keyword=None, url=None, sort_date=True, download_pdf=True):
    if not keyword and not url:
        print("Please provide a keyword or URL")
        return

    path_output = '/content/drive/MyDrive/Penalaran Komputer/UAS/CSV'
    path_pdf = '/content/drive/MyDrive/Penalaran Komputer/UAS/PDF'
    today = date.today().strftime("%Y-%m-%d")

    link = f"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1"
    if url:
        link = url

    soup = open_page(link)
    last_page = int(soup.find_all("a", {"class": "page-link"})[-1].get("data-ci-pagination-page"))

    if url:
        print(f"Scraping with url: {url} - {20 * last_page} data - {last_page} page")
    else:
        print(f"Scraping with keyword: {keyword} - {20 * last_page} data - {last_page} page")

    if url:
        keyword_url = url
    else:
        keyword_url = keyword

    futures = []
    with ThreadPoolExecutor(max_workers=4) as executor:
        for page in range(last_page):
            futures.append(
                executor.submit(run_process, keyword_url, page + 1, sort_date, path_output, path_pdf, today)
            )
    wait(futures)

# Download Pidana PN Sidoarjo
run_scraper(url="https://putusan3.mahkamahagung.go.id/search.html?q=&jenis_doc=putusan&cat=8dff1a19444a2f2d63becf72c08c2fdd&jd=&tp=&court=098167PN337&t_put=&t_reg=&t_upl=&t_pr=")

!apt install poppler-utils tesseract-ocr tesseract-ocr-ind -y
!pip install pdf2image pytesseract > /dev/null

from pdf2image import convert_from_path
import pytesseract
import os

pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"

def extract_text_ocr(pdf_path, lang='ind'):
    print(f"Membaca PDF: {os.path.basename(pdf_path)}")
    images = convert_from_path(pdf_path, dpi=300)
    full_text = ""
    for i, image in enumerate(images):
        print(f"üîé OCR halaman {i+1}/{len(images)}")
        text = pytesseract.image_to_string(image, lang=lang)
        full_text += f"\n--- Halaman {i+1} ---\n{text}"
    return full_text

from pdf2image import convert_from_path
import pytesseract
import os

# Konfigurasi folder
pdf_folder = "/content/drive/MyDrive/Penalaran Komputer/UAS/PDF"
output_folder = "/content/drive/MyDrive/Penalaran Komputer/UAS/Data/raw"
os.makedirs(output_folder, exist_ok=True)

# Log file yang sudah selesai
log_file = "/content/drive/MyDrive/Penalaran Komputer/UAS/ocr_done.txt"
pytesseract.pytesseract.tesseract_cmd = "/usr/bin/tesseract"
ocr_lang = "ind"

# Ambil file yang sudah selesai
done_files = set()
if os.path.exists(log_file):
    with open(log_file, "r") as f:
        done_files = set(f.read().splitlines())

# Ambil semua PDF
pdf_files = sorted([f for f in os.listdir(pdf_folder) if f.endswith(".pdf")])

for idx, pdf_file in enumerate(pdf_files):
    if pdf_file in done_files:
        print(f"‚è≠Ô∏è Lewati (sudah selesai): {pdf_file}")
        continue

    pdf_path = os.path.join(pdf_folder, pdf_file)
    output_name = f"ocr_{pdf_file.replace('.pdf', '.txt')}"
    output_path = os.path.join(output_folder, output_name)

    print(f"üìÑ Memproses {pdf_file} ({idx+1}/{len(pdf_files)})")

    try:
        images = convert_from_path(pdf_path, dpi=200)
        full_text = ""
        for i, image in enumerate(images):
            text = pytesseract.image_to_string(image, lang=ocr_lang)
            full_text += f"\n--- Halaman {i+1} ---\n{text}"

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(full_text)

        with open(log_file, "a") as log:
            log.write(pdf_file + "\n")

        print(f"‚úÖ Disimpan: {output_name}")
    except Exception as e:
        print(f"‚ùå Gagal proses {pdf_file}: {e}")
        break  # Stop bila ada error besar